{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abe2940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "from IPython.core.display import HTML\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afee0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_video(search_query, display_heatmap=True, display_results_count=10):\n",
    "\n",
    "  # Encode and normalize the search query using CLIP\n",
    "  with torch.no_grad():\n",
    "    text_features = model.encode_text(clip.tokenize(search_query).to(device))\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "  # Compute the similarity between the search query and each frame using the Cosine similarity\n",
    "  similarities = (100.0 * video_features @ text_features.T)\n",
    "  values, best_photo_idx = similarities.topk(display_results_count, dim=0)\n",
    "#   print(values)\n",
    "  sum_values = torch.sum(values, axis = 0)\n",
    "  ave_values = sum_values / display_results_count\n",
    "  print(ave_values)\n",
    "  test_results.append(zip(i.split('/')[-1].replace('.mp4', ''), ave_values))\n",
    "#   print(sum_values)\n",
    "#   print(type(values))\n",
    "\n",
    "  # Display the heatmap\n",
    "#   if display_heatmap:\n",
    "#     print(\"Search query heatmap over the frames of the video:\")\n",
    "#     fig = px.imshow(similarities.T.cpu().numpy(), height=50, aspect='auto', color_continuous_scale='viridis')\n",
    "#     fig.update_layout(coloraxis_showscale=False)\n",
    "#     fig.update_xaxes(showticklabels=False)\n",
    "#     fig.update_yaxes(showticklabels=False)\n",
    "#     fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "#     fig.show()\n",
    "#     print()\n",
    "\n",
    "  # Display the top 3 frames\n",
    "#   b = 0\n",
    "#   for frame_id in best_photo_idx:\n",
    "#     image = video_frames[frame_id]\n",
    "#     image.save(os.path.join(root,str(frame_id) + \".jpg\"))\n",
    "#     display(video_frames[frame_id])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Find the timestamp in the video and display it\n",
    "#     seconds = round(frame_id.cpu().numpy()[0] * N / fps)\n",
    "#     display(HTML(f\"Found at {str(datetime.timedelta(seconds=seconds))} (<a target=\\\"_blank\\\" href=\\\"{video_url}&t={seconds}\\\">link</a>)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8560f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/qls/Downloads/james_huang_project/video'\n",
    "sub_root = []\n",
    "video_1_files = []\n",
    "video_2_files = []\n",
    "video_3_files = []\n",
    "test_results = []\n",
    "N = 120\n",
    "file1 = os.listdir(root)\n",
    "for i in file1:\n",
    "    sub_root.append(os.path.join(root,i))\n",
    "for i in sub_root:\n",
    "    if i.split('/')[-1] == '1':\n",
    "        for j in os.listdir(i):\n",
    "            video_1_files.append(os.path.join(i,j))\n",
    "    if i.split('/')[-1] == '2':\n",
    "        for j in os.listdir(i):\n",
    "            video_2_files.append(os.path.join(i,j))\n",
    "    if i.split('/')[-1] == '3':\n",
    "        for j in os.listdir(i):\n",
    "            video_3_files.append(os.path.join(i,j))\n",
    "# print(video_1_files)\n",
    "# print(video_2_files)\n",
    "# print(video_3_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb057843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n",
      "1.3\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([26.1250], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([31.7031], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.4\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([27.1250], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([28.2031], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.9\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([32.3125], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.10\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([27.8281], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.7\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([23.5000], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.8\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([32.3125], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/2\n",
      "Processing batch 2/2\n",
      "1.6\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([30.0312], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "1.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([29.7031], device='cuda:0', dtype=torch.float16)\n",
      "[<zip object at 0x7fbb61641280>, <zip object at 0x7fbd425e4fc0>, <zip object at 0x7fbb61641a40>, <zip object at 0x7fbd425e4f40>, <zip object at 0x7fbb61641600>, <zip object at 0x7fbd42537ec0>, <zip object at 0x7fbd42537bc0>, <zip object at 0x7fbb6163ad80>, <zip object at 0x7fbd42537480>, <zip object at 0x7fbb5ae44600>]\n"
     ]
    }
   ],
   "source": [
    "# Load the open CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "for i in video_1_files:\n",
    "    # The frame images will be stored in video_frames\n",
    "    video_frames = []\n",
    "\n",
    "    # Open the video file\n",
    "    capture = cv2.VideoCapture(i)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    current_frame = 0\n",
    "    while capture.isOpened():\n",
    "      # Read the current frame\n",
    "      ret, frame = capture.read()\n",
    "\n",
    "      # Convert it to a PIL image (required for CLIP) and store it\n",
    "      if ret == True:\n",
    "        video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "      else:\n",
    "        break\n",
    "\n",
    "      # Skip N frames\n",
    "      current_frame += N\n",
    "      capture.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "    # Print some statistics\n",
    "#     print(f\"Frames extracted: {len(video_frames)}\")\n",
    "    \n",
    "    # You can try tuning the batch size for very large videos, but it should usually be OK\n",
    "    batch_size = 256\n",
    "    batches = math.ceil(len(video_frames) / batch_size)\n",
    "\n",
    "    # The encoded features will bs stored in video_features\n",
    "    video_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
    "\n",
    "    # Process each batch\n",
    "    for j in range(batches):\n",
    "      print(f\"Processing batch {j+1}/{batches}\")\n",
    "\n",
    "      # Get the relevant frames\n",
    "      batch_frames = video_frames[j*batch_size : (j+1)*batch_size]\n",
    "\n",
    "      # Preprocess the images for the batch\n",
    "      batch_preprocessed = torch.stack([preprocess(frame) for frame in batch_frames]).to(device)\n",
    "\n",
    "      # Encode with CLIP and normalize\n",
    "      with torch.no_grad():\n",
    "        batch_features = model.encode_image(batch_preprocessed)\n",
    "        batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      # Append the batch to the list containing all features\n",
    "      video_features = torch.cat((video_features, batch_features))\n",
    "\n",
    "    # Print some stats\n",
    "#     print(f\"Features: {video_features.shape}\")\n",
    "    \n",
    "    print(i.split('/')[-1].replace('.mp4', ''))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    search_video(\"cadillac car\")\n",
    "print(test_results)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b5c5613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n",
      "2.6\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([27.2031], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([28.8750], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.7\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([20.2188], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/6\n",
      "Processing batch 2/6\n",
      "Processing batch 3/6\n",
      "Processing batch 4/6\n",
      "Processing batch 5/6\n",
      "Processing batch 6/6\n",
      "2.10\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([28.4688], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.9\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([29.1719], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.3\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([25.5625], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([28.4219], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "2.8\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([25.6719], device='cuda:0', dtype=torch.float16)\n",
      "[<zip object at 0x7fbb61641280>, <zip object at 0x7fbd425e4fc0>, <zip object at 0x7fbb61641a40>, <zip object at 0x7fbd425e4f40>, <zip object at 0x7fbb61641600>, <zip object at 0x7fbd42537ec0>, <zip object at 0x7fbd42537bc0>, <zip object at 0x7fbb6163ad80>, <zip object at 0x7fbd42537480>, <zip object at 0x7fbb5ae44600>, <zip object at 0x7fbb5ae61b00>, <zip object at 0x7fbb45b6fb40>, <zip object at 0x7fbb5ae386c0>, <zip object at 0x7fbd426b0dc0>, <zip object at 0x7fbb5ae8e3c0>, <zip object at 0x7fbb6163f200>, <zip object at 0x7fbb5ae8e380>, <zip object at 0x7fbb61643080>]\n"
     ]
    }
   ],
   "source": [
    "for i in video_2_files:\n",
    "    # The frame images will be stored in video_frames\n",
    "    video_frames = []\n",
    "\n",
    "    # Open the video file\n",
    "    capture = cv2.VideoCapture(i)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    current_frame = 0\n",
    "    while capture.isOpened():\n",
    "      # Read the current frame\n",
    "      ret, frame = capture.read()\n",
    "\n",
    "      # Convert it to a PIL image (required for CLIP) and store it\n",
    "      if ret == True:\n",
    "        video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "      else:\n",
    "        break\n",
    "\n",
    "      # Skip N frames\n",
    "      current_frame += N\n",
    "      capture.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "    # Print some statistics\n",
    "#     print(f\"Frames extracted: {len(video_frames)}\")\n",
    "    \n",
    "    # You can try tuning the batch size for very large videos, but it should usually be OK\n",
    "    batch_size = 256\n",
    "    batches = math.ceil(len(video_frames) / batch_size)\n",
    "\n",
    "    # The encoded features will bs stored in video_features\n",
    "    video_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
    "\n",
    "    # Process each batch\n",
    "    for j in range(batches):\n",
    "      print(f\"Processing batch {j+1}/{batches}\")\n",
    "\n",
    "      # Get the relevant frames\n",
    "      batch_frames = video_frames[j*batch_size : (j+1)*batch_size]\n",
    "\n",
    "      # Preprocess the images for the batch\n",
    "      batch_preprocessed = torch.stack([preprocess(frame) for frame in batch_frames]).to(device)\n",
    "\n",
    "      # Encode with CLIP and normalize\n",
    "      with torch.no_grad():\n",
    "        batch_features = model.encode_image(batch_preprocessed)\n",
    "        batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      # Append the batch to the list containing all features\n",
    "      video_features = torch.cat((video_features, batch_features))\n",
    "\n",
    "    # Print some stats\n",
    "#     print(f\"Features: {video_features.shape}\")\n",
    "    \n",
    "    print(i.split('/')[-1].replace('.mp4', ''))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    search_video(\"anaconda\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "47e3052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1\n",
      "3.2\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([18.8438], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/2\n",
      "Processing batch 2/2\n",
      "3.10\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([16.5312], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.4\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([21.2812], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.8\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([18.9062], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.9\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([20.4062], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.5\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([18.9062], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.6\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([16.3906], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.7\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([18.6875], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/2\n",
      "Processing batch 2/2\n",
      "3.3\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([20.5469], device='cuda:0', dtype=torch.float16)\n",
      "Processing batch 1/1\n",
      "3.1\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "tensor([20.4219], device='cuda:0', dtype=torch.float16)\n",
      "[<zip object at 0x7fbb61641280>, <zip object at 0x7fbd425e4fc0>, <zip object at 0x7fbb61641a40>, <zip object at 0x7fbd425e4f40>, <zip object at 0x7fbb61641600>, <zip object at 0x7fbd42537ec0>, <zip object at 0x7fbd42537bc0>, <zip object at 0x7fbb6163ad80>, <zip object at 0x7fbd42537480>, <zip object at 0x7fbb5ae44600>, <zip object at 0x7fbb5ae61b00>, <zip object at 0x7fbb45b6fb40>, <zip object at 0x7fbb5ae386c0>, <zip object at 0x7fbd426b0dc0>, <zip object at 0x7fbb5ae8e3c0>, <zip object at 0x7fbb6163f200>, <zip object at 0x7fbb5ae8e380>, <zip object at 0x7fbb61643080>, <zip object at 0x7fbd42684140>, <zip object at 0x7fbd426bf900>, <zip object at 0x7fbd4259c940>, <zip object at 0x7fbb5ae7ab00>, <zip object at 0x7fbd4259ca00>, <zip object at 0x7fbb5ae80140>, <zip object at 0x7fbd44033100>, <zip object at 0x7fbb5ae80880>, <zip object at 0x7fbb45bb7b40>, <zip object at 0x7fbd426761c0>]\n"
     ]
    }
   ],
   "source": [
    "for i in video_3_files:\n",
    "    # The frame images will be stored in video_frames\n",
    "    video_frames = []\n",
    "\n",
    "    # Open the video file\n",
    "    capture = cv2.VideoCapture(i)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    current_frame = 0\n",
    "    while capture.isOpened():\n",
    "      # Read the current frame\n",
    "      ret, frame = capture.read()\n",
    "\n",
    "      # Convert it to a PIL image (required for CLIP) and store it\n",
    "      if ret == True:\n",
    "        video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "      else:\n",
    "        break\n",
    "\n",
    "      # Skip N frames\n",
    "      current_frame += N\n",
    "      capture.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "    # Print some statistics\n",
    "#     print(f\"Frames extracted: {len(video_frames)}\")\n",
    "    \n",
    "    # You can try tuning the batch size for very large videos, but it should usually be OK\n",
    "    batch_size = 256\n",
    "    batches = math.ceil(len(video_frames) / batch_size)\n",
    "\n",
    "    # The encoded features will bs stored in video_features\n",
    "    video_features = torch.empty([0, 512], dtype=torch.float16).to(device)\n",
    "\n",
    "    # Process each batch\n",
    "    for j in range(batches):\n",
    "      print(f\"Processing batch {j+1}/{batches}\")\n",
    "\n",
    "      # Get the relevant frames\n",
    "      batch_frames = video_frames[j*batch_size : (j+1)*batch_size]\n",
    "\n",
    "      # Preprocess the images for the batch\n",
    "      batch_preprocessed = torch.stack([preprocess(frame) for frame in batch_frames]).to(device)\n",
    "\n",
    "      # Encode with CLIP and normalize\n",
    "      with torch.no_grad():\n",
    "        batch_features = model.encode_image(batch_preprocessed)\n",
    "        batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "      # Append the batch to the list containing all features\n",
    "      video_features = torch.cat((video_features, batch_features))\n",
    "\n",
    "    # Print some stats\n",
    "#     print(f\"Features: {video_features.shape}\")\n",
    "    \n",
    "    print(i.split('/')[-1].replace('.mp4', ''))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    search_video(\"cadillac car\")\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ffb98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
